{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "512b2899-7e47-448d-92c3-e27f1d4e77c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 27 runs in project species-2024-hao-final-3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 27/27 [02:08<00:00,  4.76s/it]\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def download_wandb_logs(entity, project, output_dir=\"wandb_logs\"):\n",
    "    \"\"\"\n",
    "    Download all logs from a W&B project.\n",
    "    \n",
    "    Args:\n",
    "        entity (str): W&B entity/username\n",
    "        project (str): W&B project name\n",
    "        output_dir (str): Directory to save the logs\n",
    "    \"\"\"\n",
    "    # Initialize W&B API\n",
    "    api = wandb.Api()\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Get all runs from the project\n",
    "    runs = api.runs(f\"{entity}/{project}\")\n",
    "    \n",
    "    print(f\"Found {len(runs)} runs in project {project}\")\n",
    "    \n",
    "    for run in tqdm(runs):\n",
    "        # Create a directory for each run\n",
    "        run_dir = os.path.join(output_dir, run.name)\n",
    "        os.makedirs(run_dir, exist_ok=True)\n",
    "        \n",
    "        # Download run history as CSV\n",
    "        history_df = pd.DataFrame(run.history())\n",
    "        history_df.to_csv(os.path.join(run_dir, \"metrics.csv\"), index=False)\n",
    "        \n",
    "        # Download config as JSON\n",
    "        config_df = pd.DataFrame([run.config])\n",
    "        config_df.to_json(os.path.join(run_dir, \"config.json\"), orient=\"records\")\n",
    "        \n",
    "        # Download any files stored in the run\n",
    "        for file in run.files():\n",
    "            try:\n",
    "                file.download(root=run_dir)\n",
    "            except Exception as e:\n",
    "                print(f\"Error downloading {file.name} from run {run.name}: {e}\")\n",
    "        \n",
    "        # Save summary metrics\n",
    "        summary_df = pd.DataFrame([run.summary._json_dict])\n",
    "        summary_df.to_csv(os.path.join(run_dir, \"summary.csv\"), index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    entity = \"hmludwig\"\n",
    "    project = \"species-2024-hao-final-3\"\n",
    "    \n",
    "    download_wandb_logs(entity, project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4534c7a8-62ef-468a-ac8e-8f958be5fd0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No config.yaml found in PSO_slovenia\n",
      "Warning: No config.yaml found in DE_slovenia\n",
      "Warning: No config.yaml found in PSO_italia\n",
      "Warning: No config.yaml found in DE_italia\n",
      "Warning: No config.yaml found in CMAES_slovenia\n",
      "Warning: No config.yaml found in DE_austria\n",
      "Warning: No config.yaml found in PSO_austria\n",
      "Warning: No config.yaml found in CMAES_austria\n",
      "Warning: No config.yaml found in CMAES_italia\n",
      "Warning: No config.yaml found in plots\n",
      "\n",
      "Processing PSO_austria - 3 runs found\n",
      "Copied divine-sea-2 -> output_austria_1.log\n",
      "Copied silver-sponge-20 -> output_austria_2.log\n",
      "Copied fluent-surf-11 -> output_austria_3.log\n",
      "\n",
      "Processing CMAES_italia - 3 runs found\n",
      "Copied trim-haze-7 -> output_italia_1.log\n",
      "Copied ethereal-cloud-25 -> output_italia_2.log\n",
      "Copied decent-wind-16 -> output_italia_3.log\n",
      "\n",
      "Processing DE_italia - 3 runs found\n",
      "Copied peachy-sky-27 -> output_italia_1.log\n",
      "Copied rural-bird-9 -> output_italia_2.log\n",
      "Copied woven-snowflake-18 -> output_italia_3.log\n",
      "\n",
      "Processing CMAES_slovenia - 3 runs found\n",
      "Copied earnest-river-4 -> output_slovenia_1.log\n",
      "Copied azure-dew-22 -> output_slovenia_2.log\n",
      "Copied breezy-dust-13 -> output_slovenia_3.log\n",
      "\n",
      "Processing DE_austria - 3 runs found\n",
      "Copied resilient-mountain-12 -> output_austria_1.log\n",
      "Copied azure-dew-3 -> output_austria_2.log\n",
      "Copied sleek-cloud-21 -> output_austria_3.log\n",
      "\n",
      "Processing PSO_italia - 3 runs found\n",
      "Copied graceful-darkness-26 -> output_italia_1.log\n",
      "Copied graceful-dust-8 -> output_italia_2.log\n",
      "Copied youthful-shadow-17 -> output_italia_3.log\n",
      "\n",
      "Processing PSO_slovenia - 3 runs found\n",
      "Copied summer-feather-14 -> output_slovenia_1.log\n",
      "Copied sweet-violet-5 -> output_slovenia_2.log\n",
      "Copied devoted-durian-23 -> output_slovenia_3.log\n",
      "\n",
      "Processing CMAES_austria - 3 runs found\n",
      "Copied mild-universe-1 -> output_austria_1.log\n",
      "Copied glamorous-field-10 -> output_austria_2.log\n",
      "Copied summer-snow-19 -> output_austria_3.log\n",
      "\n",
      "Processing DE_slovenia - 3 runs found\n",
      "Copied noble-salad-24 -> output_slovenia_1.log\n",
      "Copied classic-armadillo-15 -> output_slovenia_2.log\n",
      "Copied still-glitter-6 -> output_slovenia_3.log\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "def get_region_from_heightmap(heightmap_path):\n",
    "    \"\"\"Extract region name from heightmap path.\"\"\"\n",
    "    for region in ['austria', 'italia', 'slovenia']:\n",
    "        if region in heightmap_path.lower():\n",
    "            return region\n",
    "    return None\n",
    "\n",
    "def organize_logs_by_pairs(base_dir=\"wandb_logs\"):\n",
    "    \"\"\"\n",
    "    Organize logs by algo-heightmap pairs and copy output.log files with appropriate names.\n",
    "    \"\"\"\n",
    "    # Dictionary to store runs by algo-heightmap pairs\n",
    "    pairs_dict = defaultdict(list)\n",
    "    \n",
    "    # Process each run directory\n",
    "    for run_dir in os.listdir(base_dir):\n",
    "        run_path = os.path.join(base_dir, run_dir)\n",
    "        \n",
    "        # Skip if not a directory\n",
    "        if not os.path.isdir(run_path):\n",
    "            continue\n",
    "            \n",
    "        config_path = os.path.join(run_path, \"config.yaml\")\n",
    "        if not os.path.exists(config_path):\n",
    "            print(f\"Warning: No config.yaml found in {run_dir}\")\n",
    "            continue\n",
    "            \n",
    "        # Read config file\n",
    "        with open(config_path, 'r') as f:\n",
    "            try:\n",
    "                config = yaml.safe_load(f)\n",
    "                # Extract heightmap and algo values, handling nested structure\n",
    "                heightmap = config.get('heightmap', {}).get('value', '')\n",
    "                algo = config.get('algo', {}).get('value', '')\n",
    "                \n",
    "                if heightmap and algo:\n",
    "                    # Get the region from heightmap path\n",
    "                    region = get_region_from_heightmap(heightmap)\n",
    "                    if not region:\n",
    "                        print(f\"Warning: Could not determine region from heightmap {heightmap}\")\n",
    "                        continue\n",
    "                        \n",
    "                    pair_key = f\"{algo}_{region}\"\n",
    "                    output_log = os.path.join(run_path, \"output.log\")\n",
    "                    \n",
    "                    if os.path.exists(output_log):\n",
    "                        pairs_dict[pair_key].append((run_dir, output_log, region))\n",
    "                    else:\n",
    "                        print(f\"Warning: No output.log found in {run_dir}\")\n",
    "                else:\n",
    "                    print(f\"Warning: Missing heightmap or algo in config for {run_dir}\")\n",
    "                    \n",
    "            except yaml.YAMLError as e:\n",
    "                print(f\"Error reading config.yaml in {run_dir}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    # Create directories for each pair and copy files\n",
    "    for pair_key, run_files in pairs_dict.items():\n",
    "        # Create pair directory\n",
    "        pair_dir = os.path.join(base_dir, pair_key)\n",
    "        os.makedirs(pair_dir, exist_ok=True)\n",
    "        \n",
    "        print(f\"\\nProcessing {pair_key} - {len(run_files)} runs found\")\n",
    "        \n",
    "        # Copy output.log files with region-based names\n",
    "        for i, (run_name, log_path, region) in enumerate(run_files):\n",
    "            new_name = f\"output_{region}_{i+1}.log\"\n",
    "            dest_path = os.path.join(pair_dir, new_name)\n",
    "            \n",
    "            try:\n",
    "                shutil.copy2(log_path, dest_path)\n",
    "                print(f\"Copied {run_name} -> {new_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error copying {log_path}: {e}\")\n",
    "        \n",
    "        if len(run_files) != 3:\n",
    "            print(f\"Warning: {pair_key} has {len(run_files)} runs (expected 3)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    organize_logs_by_pairs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73003ad6-6c31-45ba-b11f-7392e28c7bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting table from wandb_logs/PSO_slovenia/output_slovenia_2.log to wandb_logs/PSO_slovenia/output_slovenia_2.csv\n",
      "Extracting table from wandb_logs/PSO_slovenia/output_slovenia_1.log to wandb_logs/PSO_slovenia/output_slovenia_1.csv\n",
      "Extracting table from wandb_logs/PSO_slovenia/output_slovenia_3.log to wandb_logs/PSO_slovenia/output_slovenia_3.csv\n",
      "Extracting table from wandb_logs/DE_slovenia/output_slovenia_2.log to wandb_logs/DE_slovenia/output_slovenia_2.csv\n",
      "Extracting table from wandb_logs/DE_slovenia/output_slovenia_1.log to wandb_logs/DE_slovenia/output_slovenia_1.csv\n",
      "Extracting table from wandb_logs/DE_slovenia/output_slovenia_3.log to wandb_logs/DE_slovenia/output_slovenia_3.csv\n",
      "Extracting table from wandb_logs/PSO_italia/output_italia_3.log to wandb_logs/PSO_italia/output_italia_3.csv\n",
      "Extracting table from wandb_logs/PSO_italia/output_italia_2.log to wandb_logs/PSO_italia/output_italia_2.csv\n",
      "Extracting table from wandb_logs/PSO_italia/output_italia_1.log to wandb_logs/PSO_italia/output_italia_1.csv\n",
      "Extracting table from wandb_logs/DE_italia/output_italia_3.log to wandb_logs/DE_italia/output_italia_3.csv\n",
      "Extracting table from wandb_logs/DE_italia/output_italia_2.log to wandb_logs/DE_italia/output_italia_2.csv\n",
      "Extracting table from wandb_logs/DE_italia/output_italia_1.log to wandb_logs/DE_italia/output_italia_1.csv\n",
      "Extracting table from wandb_logs/CMAES_slovenia/output_slovenia_2.log to wandb_logs/CMAES_slovenia/output_slovenia_2.csv\n",
      "Extracting table from wandb_logs/CMAES_slovenia/output_slovenia_1.log to wandb_logs/CMAES_slovenia/output_slovenia_1.csv\n",
      "Extracting table from wandb_logs/CMAES_slovenia/output_slovenia_3.log to wandb_logs/CMAES_slovenia/output_slovenia_3.csv\n",
      "Extracting table from wandb_logs/DE_austria/output_austria_2.log to wandb_logs/DE_austria/output_austria_2.csv\n",
      "Extracting table from wandb_logs/DE_austria/output_austria_1.log to wandb_logs/DE_austria/output_austria_1.csv\n",
      "Extracting table from wandb_logs/DE_austria/output_austria_3.log to wandb_logs/DE_austria/output_austria_3.csv\n",
      "Extracting table from wandb_logs/PSO_austria/output_austria_2.log to wandb_logs/PSO_austria/output_austria_2.csv\n",
      "Extracting table from wandb_logs/PSO_austria/output_austria_1.log to wandb_logs/PSO_austria/output_austria_1.csv\n",
      "Extracting table from wandb_logs/PSO_austria/output_austria_3.log to wandb_logs/PSO_austria/output_austria_3.csv\n",
      "Extracting table from wandb_logs/CMAES_austria/output_austria_2.log to wandb_logs/CMAES_austria/output_austria_2.csv\n",
      "Extracting table from wandb_logs/CMAES_austria/output_austria_1.log to wandb_logs/CMAES_austria/output_austria_1.csv\n",
      "Extracting table from wandb_logs/CMAES_austria/output_austria_3.log to wandb_logs/CMAES_austria/output_austria_3.csv\n",
      "Extracting table from wandb_logs/CMAES_austria/.ipynb_checkpoints/output_austria_3-checkpoint.log to wandb_logs/CMAES_austria/.ipynb_checkpoints/output_austria_3-checkpoint.csv\n",
      "Extracting table from wandb_logs/CMAES_italia/output_italia_3.log to wandb_logs/CMAES_italia/output_italia_3.csv\n",
      "Extracting table from wandb_logs/CMAES_italia/output_italia_2.log to wandb_logs/CMAES_italia/output_italia_2.csv\n",
      "Extracting table from wandb_logs/CMAES_italia/output_italia_1.log to wandb_logs/CMAES_italia/output_italia_1.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "def extract_cmaes_table(log_content):\n",
    "    \"\"\"Extract the CMAES table from log content and return as list of rows.\"\"\"\n",
    "    # Find the table header\n",
    "    header_match = re.search(r'====+\\n(n_gen.*?)\\n====+\\n', log_content, re.DOTALL)\n",
    "    if not header_match:\n",
    "        return None, None\n",
    "    \n",
    "    header = header_match.group(1).strip()\n",
    "    headers = [h.strip() for h in header.split('|')]\n",
    "    \n",
    "    # Find all table rows (looking for lines with numbers and pipe symbols)\n",
    "    table_pattern = r'\\s*\\d+\\s*\\|\\s*\\d+\\s*\\|.*?\\n'\n",
    "    rows = re.findall(table_pattern, log_content)\n",
    "    \n",
    "    if not rows:\n",
    "        return None, None\n",
    "        \n",
    "    # Process each row\n",
    "    processed_rows = []\n",
    "    for row in rows:\n",
    "        # Split by pipe and strip whitespace\n",
    "        values = [cell.strip() for cell in row.split('|')]\n",
    "        processed_rows.append(values)\n",
    "        \n",
    "    return headers, processed_rows\n",
    "\n",
    "def process_logs(base_dir=\"wandb_logs\"):\n",
    "    \"\"\"Process all log files and extract CMAES tables to CSVs.\"\"\"\n",
    "    for root, dirs, files in os.walk(base_dir):\n",
    "        for file in files:\n",
    "            if file.startswith('output_') and file.endswith('.log'):\n",
    "                log_path = os.path.join(root, file)\n",
    "                csv_path = log_path.rsplit('.', 1)[0] + '.csv'\n",
    "                \n",
    "                try:\n",
    "                    with open(log_path, 'r') as f:\n",
    "                        content = f.read()\n",
    "                        \n",
    "                    headers, rows = extract_cmaes_table(content)\n",
    "                    \n",
    "                    if headers and rows:\n",
    "                        print(f\"Extracting table from {log_path} to {csv_path}\")\n",
    "                        with open(csv_path, 'w', newline='') as f:\n",
    "                            writer = csv.writer(f)\n",
    "                            writer.writerow(headers)\n",
    "                            writer.writerows(rows)\n",
    "                    else:\n",
    "                        print(f\"No CMAES table found in {log_path}\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {log_path}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c922eb15-442b-4bb8-856c-6bb5509971da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing directory: PSO_slovenia\n",
      "Created combined file: wandb_logs/PSO_slovenia/PSO_slovenia_combined.csv\n",
      "Created statistics file: wandb_logs/PSO_slovenia/PSO_slovenia_stats.csv\n",
      "\n",
      "Processing directory: DE_slovenia\n",
      "Created combined file: wandb_logs/DE_slovenia/DE_slovenia_combined.csv\n",
      "Created statistics file: wandb_logs/DE_slovenia/DE_slovenia_stats.csv\n",
      "\n",
      "Processing directory: PSO_italia\n",
      "Created combined file: wandb_logs/PSO_italia/PSO_italia_combined.csv\n",
      "Created statistics file: wandb_logs/PSO_italia/PSO_italia_stats.csv\n",
      "\n",
      "Processing directory: DE_italia\n",
      "Created combined file: wandb_logs/DE_italia/DE_italia_combined.csv\n",
      "Created statistics file: wandb_logs/DE_italia/DE_italia_stats.csv\n",
      "\n",
      "Processing directory: CMAES_slovenia\n",
      "Created combined file: wandb_logs/CMAES_slovenia/CMAES_slovenia_combined.csv\n",
      "Created statistics file: wandb_logs/CMAES_slovenia/CMAES_slovenia_stats.csv\n",
      "\n",
      "Processing directory: DE_austria\n",
      "Created combined file: wandb_logs/DE_austria/DE_austria_combined.csv\n",
      "Created statistics file: wandb_logs/DE_austria/DE_austria_stats.csv\n",
      "\n",
      "Processing directory: PSO_austria\n",
      "Created combined file: wandb_logs/PSO_austria/PSO_austria_combined.csv\n",
      "Created statistics file: wandb_logs/PSO_austria/PSO_austria_stats.csv\n",
      "\n",
      "Processing directory: CMAES_austria\n",
      "Created combined file: wandb_logs/CMAES_austria/CMAES_austria_combined.csv\n",
      "Created statistics file: wandb_logs/CMAES_austria/CMAES_austria_stats.csv\n",
      "\n",
      "Processing directory: .ipynb_checkpoints\n",
      "Created combined file: wandb_logs/CMAES_austria/.ipynb_checkpoints/.ipynb_checkpoints_combined.csv\n",
      "Created statistics file: wandb_logs/CMAES_austria/.ipynb_checkpoints/.ipynb_checkpoints_stats.csv\n",
      "\n",
      "Processing directory: CMAES_italia\n",
      "Created combined file: wandb_logs/CMAES_italia/CMAES_italia_combined.csv\n",
      "Created statistics file: wandb_logs/CMAES_italia/CMAES_italia_stats.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "def combine_metrics(base_dir=\"wandb_logs\"):\n",
    "    \"\"\"\n",
    "    Combine f_min and f_avg from all CSV files in each algo_map directory.\n",
    "    Creates a summary CSV with all runs side by side.\n",
    "    \"\"\"\n",
    "    # Walk through the base directory\n",
    "    for root, dirs, files in os.walk(base_dir):\n",
    "        csv_files = [f for f in files if f.endswith('.csv') and f.startswith('output_')]\n",
    "        \n",
    "        # Skip if no CSV files found\n",
    "        if not csv_files:\n",
    "            continue\n",
    "            \n",
    "        # Get algo and map from directory name\n",
    "        dir_name = os.path.basename(root)\n",
    "        if '_' not in dir_name:  # Skip if not an algo_map directory\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nProcessing directory: {dir_name}\")\n",
    "        \n",
    "        # Initialize DataFrames dictionary for this directory\n",
    "        runs_data = {}\n",
    "        \n",
    "        # Process each CSV file\n",
    "        for csv_file in sorted(csv_files):\n",
    "            try:\n",
    "                # Read the CSV\n",
    "                df = pd.read_csv(os.path.join(root, csv_file))\n",
    "                \n",
    "                # Extract run number from filename\n",
    "                run_num = csv_file.split('_')[-1].replace('.csv', '')\n",
    "                \n",
    "                # Store f_min and f_avg with run number\n",
    "                runs_data[f'f_min_run_{run_num}'] = df['f_min'].values\n",
    "                runs_data[f'f_avg_run_{run_num}'] = df['f_avg'].values\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {csv_file}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if runs_data:\n",
    "            # Create combined DataFrame\n",
    "            combined_df = pd.DataFrame(runs_data)\n",
    "            \n",
    "            # Add generation number\n",
    "            combined_df.insert(0, 'generation', range(1, len(combined_df) + 1))\n",
    "            \n",
    "            # Save combined data\n",
    "            output_file = os.path.join(root, f'{dir_name}_combined.csv')\n",
    "            combined_df.to_csv(output_file, index=False)\n",
    "            print(f\"Created combined file: {output_file}\")\n",
    "            \n",
    "            # Calculate and save statistics\n",
    "            stats_df = pd.DataFrame()\n",
    "            stats_df['generation'] = combined_df['generation']\n",
    "            \n",
    "            # Calculate mean and std for f_min and f_avg across runs\n",
    "            f_min_cols = [col for col in combined_df.columns if col.startswith('f_min')]\n",
    "            f_avg_cols = [col for col in combined_df.columns if col.startswith('f_avg')]\n",
    "            \n",
    "            stats_df['f_min_mean'] = combined_df[f_min_cols].mean(axis=1)\n",
    "            stats_df['f_min_std'] = combined_df[f_min_cols].std(axis=1)\n",
    "            stats_df['f_avg_mean'] = combined_df[f_avg_cols].mean(axis=1)\n",
    "            stats_df['f_avg_std'] = combined_df[f_avg_cols].std(axis=1)\n",
    "            \n",
    "            # Save statistics\n",
    "            stats_file = os.path.join(root, f'{dir_name}_stats.csv')\n",
    "            stats_df.to_csv(stats_file, index=False)\n",
    "            print(f\"Created statistics file: {stats_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    combine_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef15410f-027b-4791-a41f-9630ea91f0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created combined performance plot for slovenia\n",
      "Created combined performance plot for italy\n",
      "Created combined performance plot for austria\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "\n",
    "def create_performance_plots(base_dir=\"wandb_logs\"):\n",
    "    \"\"\"\n",
    "    Create combined plots for f_avg and f_min, using consistent colors for each algorithm\n",
    "    across all maps.\n",
    "    \"\"\"\n",
    "    # Set style for publication-quality plots\n",
    "    plt.style.use('seaborn-v0_8-paper')\n",
    "    sns.set_palette(\"deep\")\n",
    "    plt.rcParams.update({\n",
    "        'font.size': 11,\n",
    "        'font.family': 'serif',\n",
    "        'axes.labelsize': 12,\n",
    "        'axes.titlesize': 12,\n",
    "        'xtick.labelsize': 10,\n",
    "        'ytick.labelsize': 10,\n",
    "        'legend.fontsize': 10,\n",
    "        'figure.figsize': (8, 5),\n",
    "        'figure.dpi': 300,\n",
    "        'lines.linewidth': 2\n",
    "    })\n",
    "\n",
    "    # First pass: collect all unique algorithms\n",
    "    all_algorithms = set()\n",
    "    map_data = {}\n",
    "    \n",
    "    for root, _, files in os.walk(base_dir):\n",
    "        # Skip .ipynb_checkpoints directories\n",
    "        if '.ipynb_checkpoints' in root:\n",
    "            continue\n",
    "            \n",
    "        for file in files:\n",
    "            if file.endswith('_stats.csv'):\n",
    "                algo_map = os.path.basename(root)\n",
    "                if '_' not in algo_map:\n",
    "                    continue\n",
    "                \n",
    "                # Split on last underscore to handle algorithms with underscores in their names\n",
    "                algo = '_'.join(algo_map.split('_')[:-1])\n",
    "                map_name = algo_map.split('_')[-1]\n",
    "                \n",
    "                if map_name not in map_data:\n",
    "                    map_data[map_name] = {}\n",
    "                \n",
    "                # Read stats file\n",
    "                df = pd.read_csv(os.path.join(root, file))\n",
    "                map_data[map_name][algo] = df\n",
    "                all_algorithms.add(algo)\n",
    "\n",
    "    # Create consistent color mapping\n",
    "    algorithms = sorted(list(all_algorithms))  # Sort for consistency\n",
    "    colors = sns.color_palette(\"deep\", n_colors=len(algorithms))\n",
    "    algo_colors = dict(zip(algorithms, colors))\n",
    "\n",
    "    # Create combined plots for each map\n",
    "    for map_name, algo_data in map_data.items():\n",
    "        # Create output directory\n",
    "        output_dir = os.path.join(base_dir, 'plots')\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Create combined plot\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        \n",
    "        for algo, data in sorted(algo_data.items()):  # Sort algorithms for consistent legend order\n",
    "            generations = data['generation']\n",
    "            color = algo_colors[algo]\n",
    "            \n",
    "            # Plot f_min with solid line\n",
    "            plt.plot(generations, data['f_min_mean'], \n",
    "                    label=f'{algo} (Best)',\n",
    "                    color=color,\n",
    "                    linewidth=2)\n",
    "            plt.fill_between(generations,\n",
    "                           data['f_min_mean'] - data['f_min_std'],\n",
    "                           data['f_min_mean'] + data['f_min_std'],\n",
    "                           color=color,\n",
    "                           alpha=0.1)\n",
    "            \n",
    "            # Plot f_avg with dashed line\n",
    "            plt.plot(generations, data['f_avg_mean'], \n",
    "                    label=f'{algo} (Avg)',\n",
    "                    color=color,\n",
    "                    linestyle='--',\n",
    "                    linewidth=1.5)\n",
    "            plt.fill_between(generations,\n",
    "                           data['f_avg_mean'] - data['f_avg_std'],\n",
    "                           data['f_avg_mean'] + data['f_avg_std'],\n",
    "                           color=color,\n",
    "                           alpha=0.1)\n",
    "        \n",
    "        plt.xlabel('Generation')\n",
    "        plt.ylabel('Fitness')\n",
    "        if map_name == \"italia\":\n",
    "            map_name = \"italy\"\n",
    "        plt.title(f'{map_name.capitalize()}')\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # Adjust legend to be more compact\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save combined plot in both formats\n",
    "        plt.savefig(os.path.join(output_dir, f'combined_fitness_{map_name}.png'), \n",
    "                   bbox_inches='tight', \n",
    "                   dpi=300)\n",
    "        plt.savefig(os.path.join(output_dir, f'combined_fitness_{map_name}.pdf'), \n",
    "                   bbox_inches='tight', \n",
    "                   format='pdf')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"Created combined performance plot for {map_name}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_performance_plots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2780a7aa-d4d4-4f83-9c65-631c4f7f5e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created combined plot for all maps\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "\n",
    "def create_combined_performance_plot(base_dir=\"wandb_logs\"):\n",
    "    \"\"\"\n",
    "    Create a single figure with 1x3 subplots for all maps, sharing y-axis and having a single legend.\n",
    "    \"\"\"\n",
    "    # Set style for publication-quality plots\n",
    "    plt.style.use('seaborn-v0_8-paper')\n",
    "    sns.set_palette(\"deep\")\n",
    "    plt.rcParams.update({\n",
    "        'font.size': 11,\n",
    "        'font.family': 'serif',\n",
    "        'axes.labelsize': 12,\n",
    "        'axes.titlesize': 12,\n",
    "        'xtick.labelsize': 10,\n",
    "        'ytick.labelsize': 10,\n",
    "        'legend.fontsize': 10,\n",
    "    })\n",
    "\n",
    "    # First pass: collect all unique algorithms and data\n",
    "    all_algorithms = set()\n",
    "    map_data = {}\n",
    "    \n",
    "    for root, _, files in os.walk(base_dir):\n",
    "        # Skip .ipynb_checkpoints directories\n",
    "        if '.ipynb_checkpoints' in root:\n",
    "            continue\n",
    "            \n",
    "        for file in files:\n",
    "            if file.endswith('_stats.csv'):\n",
    "                algo_map = os.path.basename(root)\n",
    "                if '_' not in algo_map:\n",
    "                    continue\n",
    "                \n",
    "                algo = '_'.join(algo_map.split('_')[:-1])\n",
    "                map_name = algo_map.split('_')[-1]\n",
    "                \n",
    "                if map_name not in map_data:\n",
    "                    map_data[map_name] = {}\n",
    "                \n",
    "                # Read stats file\n",
    "                df = pd.read_csv(os.path.join(root, file))\n",
    "                map_data[map_name][algo] = df\n",
    "                all_algorithms.add(algo)\n",
    "\n",
    "    # Create color mapping\n",
    "    algorithms = sorted(list(all_algorithms))\n",
    "    colors = sns.color_palette(\"deep\", n_colors=len(algorithms))\n",
    "    algo_colors = dict(zip(algorithms, colors))\n",
    "\n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5), sharey=False)  # Changed sharey to False\n",
    "    fig.subplots_adjust(wspace=0.15)  # Adjusted spacing between subplots\n",
    "\n",
    "    # Define the order of maps\n",
    "    map_order = ['slovenia', 'austria', 'italia']\n",
    "    \n",
    "    # Store handles and labels for legend\n",
    "    legend_handles = []\n",
    "    legend_labels = []\n",
    "\n",
    "    # Plot each map\n",
    "    for idx, map_name in enumerate(map_order):\n",
    "        if map_name not in map_data:\n",
    "            continue\n",
    "            \n",
    "        ax = axes[idx]\n",
    "        \n",
    "        for algo, data in sorted(map_data[map_name].items()):\n",
    "            generations = data['generation']\n",
    "            color = algo_colors[algo]\n",
    "            \n",
    "            # Plot f_min with solid line\n",
    "            line_min = ax.plot(generations, data['f_min_mean'],\n",
    "                             color=color,\n",
    "                             linewidth=2)\n",
    "            ax.fill_between(generations,\n",
    "                          data['f_min_mean'] - data['f_min_std'],\n",
    "                          data['f_min_mean'] + data['f_min_std'],\n",
    "                          color=color,\n",
    "                          alpha=0.1)\n",
    "            \n",
    "            # Plot f_avg with dashed line\n",
    "            line_avg = ax.plot(generations, data['f_avg_mean'],\n",
    "                             color=color,\n",
    "                             linestyle='--',\n",
    "                             linewidth=1.5)\n",
    "            ax.fill_between(generations,\n",
    "                          data['f_avg_mean'] - data['f_avg_std'],\n",
    "                          data['f_avg_mean'] + data['f_avg_std'],\n",
    "                          color=color,\n",
    "                          alpha=0.1)\n",
    "            \n",
    "            # Add to legend only for the first subplot\n",
    "            if idx == 0:\n",
    "                legend_handles.extend([line_min[0], line_avg[0]])\n",
    "                legend_labels.extend([f'{algo} (Best)', f'{algo} (Avg)'])\n",
    "\n",
    "        ax.set_xlabel('Generation')\n",
    "        if idx == 0:\n",
    "            ax.set_ylabel('Fitness')\n",
    "        if map_name == \"italia\":\n",
    "            map_name = \"italy\"\n",
    "        ax.set_title(f'{map_name.capitalize()}')\n",
    "        ax.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "    # Add single legend to the right of the plots\n",
    "    fig.legend(legend_handles, legend_labels,\n",
    "              bbox_to_anchor=(1.02, 0.5),\n",
    "              loc='center left',\n",
    "              borderaxespad=0)\n",
    "\n",
    "    # Adjust layout to prevent legend overlap\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Create output directory and save plots\n",
    "    output_dir = os.path.join(base_dir, 'plots')\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    plt.savefig(os.path.join(output_dir, 'combined_maps.png'),\n",
    "                bbox_inches='tight',\n",
    "                dpi=300)\n",
    "    plt.savefig(os.path.join(output_dir, 'combined_maps.pdf'),\n",
    "                bbox_inches='tight',\n",
    "                format='pdf')\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"Created combined plot for all maps\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_combined_performance_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae07cbb-dd77-4eb8-9fbe-f8a32814e66a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
